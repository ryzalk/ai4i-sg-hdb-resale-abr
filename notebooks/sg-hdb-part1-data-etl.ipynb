{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://gallery.mailchimp.com/f98d5ac0a3fbbdcdda35136ab/images/2002af76-5fd4-4185-9d49-28558b6b8772.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `sg-hdb-resale-abr` \n",
    "# Part 1: Extract, Transform, Load\n",
    "\n",
    "In this notebook, I will be carry out steps and constructing structures that allows the following:\n",
    "+ preliminary exploration of the raw data in terms of its state and format that it comes in\n",
    "+ storing of extracted data in variables\n",
    "+ loading extracted data into a database\n",
    "\n",
    "These things takes one through the **ETL** process that is facilitated by a set of functions that makes a data pipeline. Albeit a simple one, beginners can learn from this whole process that we're about to go through.\n",
    "\n",
    "<img src=\"https://i.ibb.co/wJQ4fK7/etl-workflow-image.png\">\n",
    "\n",
    "What is **ETL**?\n",
    "+ **Extract:** This is the process of extracting data/information from the raw files. In our context here, the raw files have been provided to us in CSV form. In other enterprise use cases these raw files can come in other forms such as streamed JSON objects or transactional data from OLTP databases.\n",
    "+ **Transform:** The process of converting data from the aforementioned extraction process to a digestible format to be ingested to another database or a datalake.\n",
    "+ **Load:** Following transformation where the extracted data has been reformated, the process of loading all of it into a database comes under here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin by importing the packages we need\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"><h1><center>Extract</center></h1></font>\n",
    "Download data at:\n",
    "+ https://data.gov.sg/dataset/resale-flat-prices\n",
    "\n",
    "__Resale Flat Prices:__ This dataset consist of transactions for HDB resale units.\n",
    "\n",
    "This section is where we carry out the main objective of extracting the data from the CSV files containing information regarding HDB resale units. First, let us create a list containing the names of the .csv files within the directory containing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set destination of folder containing raw data\n",
    "\n",
    "# This variable will contain every file in folder\n",
    "\n",
    "# This variable only bothers with files ending with '.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function that we will be creating below is to combine all the data from all the .csv files into one `pandas` dataframe. We will be using the `pandas` function `concat` to combine. Where a dataset does not have a certain variable that exists in the other dataset, the variable will be retained while filling in '0's for empty values. This is specifically referring to the variable `remaining_lease`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilise the newly created functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the first 5 observations of dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"><h1><center>Transform</center></h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we have extracted from the CSV files are quite clean and hence we can choose to not do any transformation prior to the loading process. Of course, in the real world, hardly ever do we get such luck.\n",
    "Further transformations for the purpose of feature engineering can be implemented during the [modelling phase](./sg-hdb-part2-modelling.ipynb).\n",
    "\n",
    "Even though the formatting/state of the dataset is good enough for us to ingest into a database, for the purpose of this exercise, let us transform the values of a single variable.\n",
    "\n",
    "Currently, as seen below, the variable `flat_model` contains many (35) different categories and some are mismatched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique values for the variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display no. of categories for the variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have many different categories but some of them are linked to the same one category and are actually just spelled differently due to entry methods. For example we have the following categories as observed from above:\n",
    "+ 'Model A'\n",
    "+ 'MODEL A'\n",
    "\n",
    "Both are pertaining to a single model category but due to the different casings they are treated as different categories. A simple act of transformation that we can employ is to just convert every letter of the values in the `flat_model` column to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique values for the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display no. of categories for the variable after transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, through transformation we are able to handle mismatched categories and in this sense, we have only done some form of preliminary data cleaning but that of course does not deviate from the essence of the transformation process.\n",
    "\n",
    "Let us now export the extracted data to one single .csv file for checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include index for dataframe and renaming the column to 'id'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the 'index' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"><h1><center>Load</center></h1></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following extraction and transformation, we now intend to load the data derived from the above processes into a simple [SQLite](https://www.sqlite.org/index.html) RDMS/database. \n",
    "(For simplicity's sake, we'll use SQLite for now. In the future, one might want to take a look into remote alternatives.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import sqlalchemy\n",
    "from sqlalchemy import Table, Column, Integer, String, Float\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick observation on how the dataset that we intend to load into the database looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb_combi_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next few cells, we will be working towards creating the database:\n",
    "+ create engine to initialise connection\n",
    "+ specify table names and their columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine\n",
    "engine = create_engine('sqlite:///../data/processed/sg_hdb.db')\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify properties for tables\n",
    "class HDBRes(Base):\n",
    "    # Specifying the table name\n",
    "    __tablename__ = 'sg_hdb_resale'\n",
    "    \n",
    "    # Specifying the column headings and types for this table\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    block = Column(String(7))\n",
    "    flat_model = Column(String(30))\n",
    "    flat_type = Column(String(20))\n",
    "    floor_area_sqm = Column(Float())\n",
    "    lease_commence_date = Column(Integer())\n",
    "    month = Column(String(7))\n",
    "    remaining_lease = Column(Integer())\n",
    "    resale_price = Column(Float())\n",
    "    storey_range = Column(String(15))\n",
    "    street_name = Column(String(50))\n",
    "    town = Column(String(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables as defined above\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a function that allows us to connect with the database created from above and insert values from relevant `pandas` dataframes into the SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGHDBBulkInsert(table_name, df_to_insert, engine_loc):\n",
    "    engine = create_engine(engine_loc)\n",
    "    \n",
    "    # The orient='records' is the key of this, it allows to align with the format mentioned in the doc to insert in bulks.\n",
    "    list_to_write = df_to_insert.to_dict(orient='records')\n",
    "    metadata = sqlalchemy.schema.MetaData(bind=engine)\n",
    "    table = sqlalchemy.Table(table_name, metadata, autoload=True)\n",
    "    \n",
    "    # Open the session\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    \n",
    "    conn = engine.connect()\n",
    "    # Insert the dataframe into the database in one bulk\n",
    "    conn.execute(table.insert(), list_to_write)\n",
    "    # Commit the changes\n",
    "    session.commit()\n",
    "    # Close the session\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing insertion of the HDB Resale data\n",
    "SGHDBBulkInsert('sg_hdb_resale', hdb_combi_df, 'sqlite:///../data/processed/sg_hdb.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe as to whether the intended operations have been executed successfully, we can use relevant GUI tools to examine the contents of databases. For SQLite, we can use [DB Browser for SQLite](https://sqlitebrowser.org/dl/). Once we have loaded the relevant data into our database, it is time for us to work on a simple machine learning model. On to the next part [here](./sg-hdb-part2-modelling.ipynb)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
